<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks">
  <meta name="keywords" content="Language-Conditioned Robotic Manipulation, Benchmark, Dataset">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title style="letter-spacing: 0.1em; font-variant: small-caps;">VLABench</title>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R2ZLMKHR2E"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-R2ZLMKHR2E');
</script>

  <script>
    function updateInteractive() {
      var task = document.getElementById("interactive-menu").value;

      console.log("interactive", task)
      // Update Video
      var video_base = document.getElementById("video-base");
      video_base.src = "./media/primitive_task/" + 
                  task + "/" +
                  "base.mp4"
      video_base.play();

      var video_spatial = document.getElementById("video-spatial");
      video_spatial.src = "./media/primitive_task/" + 
                  task + "/" +
                  "spatial.mp4"
      video_spatial.play();

      var video_commonsense = document.getElementById("video-commonsense");
      video_commonsense.src = "./media/primitive_task/" + 
                  task + "/" +
                  "commonsense.mp4"
      video_commonsense.play();

      var video_semantic = document.getElementById("video-semantic");
      video_semantic.src = "./media/primitive_task/" + 
                  task + "/" +
                  "semantic.mp4"
      video_semantic.play();

    // Update Text Description
    fetch("./media/primitive_task/" + task + "/" + "base.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load base text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("base-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("base-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "spatial.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load spatial text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("spatial-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("spatial-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "commonsense.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load commonsense text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("commonsense-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("commonsense-text").innerText = "Error loading description.";
    });

    fetch("./media/primitive_task/" + task + "/" + "semantic.txt")
    .then(response => {
      if (!response.ok) {
        throw new Error("Failed to load semantic text for task: " + task);
      }
      return response.text();
    })
    .then(text => {
      document.getElementById("semantic-text").innerText = "Instruction: " + text;
    })
    .catch(error => {
      console.error(error);
      document.getElementById("semantic-text").innerText = "Error loading description.";
    });
    }


  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Shiduo Zhang<sup>1</sup>,
            </span>
            <span class="author-block">
              Zhe Xu<sup>1</sup>,
            </span>
            <span class="author-block">
              Peiju Liu<sup>1*</sup>,
            </span>
            <span class="author-block">
              Xiaopeng Yu<sup>1*</sup>,
            </span>
            <span class="author-block">
              Yuan Li<sup>1</sup>,
            </span>
            <span class="author-block">
              Qinghui Gao<sup>1</sup>,
            </span>
        </br>
            <span class="author-block">
              Zhaoye Fei<sup>1</sup>,
            </span>
            <span class="author-block">
              Zhangyue Yin<sup>1</sup>,
            </span>
            <span class="author-block">
              Zuxuan Wu<sup>1</sup>,
            </span>
            <span class="author-block">
              Yugang Jiang<sup>1</sup>,
              </span>
              <span class="author-block">
                Xipeng Qiu<sup>1</sup>
              </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>School of Computer Science and Technology, Fudan University. </span>
            </br>
            <span class="author-block"><sup>*</sup>Equal Contribution. </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>Arxiv</span>
              </a>
            </span>
            <!-- <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" width="50"> -->
            <!-- Video Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://github.com"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video(On the way)</span>
              </a>
            </span> -->

            <!-- Hugging face Link -->
            <span class="link-block">
              <a target="_blank" href="https://github.com"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                  alt="Hugging Face" width="24" style="vertical-align: middle;">
                </span>
                <span>Demo Dataset</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-bottom: 1em;">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">
    <!-- Section Title -->
    <div class="has-text-centered" style="margin-bottom: 2em;">
      <h1 class="title is-3">VLABench</h1>
      <h2 class="title is-4">New Definition for LCM Tasks Suitable for Foundation Models</h2>
      <h2 class="subtitle is-5">——What Abilities Should True a VLA Have?</h2>
    </div>

    <!-- Content with Image and Text -->
    <div class="columns is-vcentered">
      <!-- Left Column: Image Placeholder
      <div class="column is-half has-text-centered">
        <div class="box" style="height: 300px; background-color: #f5f5f5; border: 1px dashed #ccc;">
          <p class="is-size-5 has-text-grey">Image Placeholder</p>
        </div>
      </div> -->

      <!-- Right Column: Text Placeholder -->
     
        <div class="content">
          <p class="is-size-5">
            From the perspective of “intelligence”, it is divided into six capability dimensions:
          </p>
          <ul>
            <li><span style="font-weight: bolder;">Mesh & Texture Understanding.</span>
              It should be able to recognize irregular and uniquely shaped meshes as well as diversified textures with rich semantic information. This involves basic open-vocabulary object recognition, OCR capabilities and etc.
            </li>
            <li><span style="font-weight: bolder;">Spatial Understanding.</span>
              It should possess basic spatial perception abilities, enabling accurate judgment of the <span style="font-weight: bolder;">relative positions</span> of objects in an image,<span style="font-weight: bolder;">spatial constraints</span> between different objects, and even direct <span style="font-weight: bolder;">distance estimation</span>.
            </li>
            <li><span style="font-weight: bolder;">Common Sense & World Knowledge Transfer.</span>
              It should acquire world knowledge and common sense from large-scale pretraining and apply such priors to corresponding tasks. For example, associating visual information with world knowledge to align it with user requirements.
            </li>
            <li><span style="font-weight: bolder;">Semantic Instruction Understanding.</span>
              It should retain strong language comprehension abilities, enabling it to extract user needs from natural interactions or understand the implicit goals of a task, and then execute dynamic action sequences. Instead of template instructions like “pick A and then place it to B”.
            </li>
            <li><span style="font-weight: bolder;">Physical Laws Understanding.</span>
              It should understand the principles of the physical world, such as friction, gravity, acceleration, and even fundamental physical concepts like the lever principle.
            </li>
            <li><span style="font-weight: bolder;">Long-Horizon Reasoning.</span> 
              Reasoning here primarily refers to the ability to plan for long-horizon, multi-step tasks, where logical correlations between multiple action steps are required. Broader reasoning encompasses several of the aforementioned abilities, such as semantic inference, the incorporation of world knowledge, and alignment between vision and task objectives. However, in this context, we focus solely on the former.
            </li>
          </ul>
        </div>
    </div>
  </div>
</section>

<!-- Primitive Display -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="container has-text-centered">
      <h1 class="title is-4" style="margin-bottom: 1em;">100 Tasks in VLABench</h1>
      <h2 class="subtitle is-5" style="margin-bottom: 1em;">——What Tasks Should a True VLA Do?</h2>
    </div>
    <div>
      <p>
        VLABench divides tasks into two categories: Primitive and Composite.
      </p>
      <ul>
        <li><span style="font-weight: bolder;">Primitive Tasks:</span> 60 tasks that require only one or two dimensions abilities and few skill combinations.</li>
        <li><span style="font-weight: bolder;">Composite Tasks:</span> 40 tasks that require multi-step reasoning and long-horizon planning, involving more skills and abilities.</li>
      </ul>
      <scan style="line-height: 1.5; font-style: italic;">The 100 task categories are just a starting point. E.g. New tasks can be created by arbitrarily combining the above-mentioned abilities and skills.</scan>
    </div>
    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- Interactive Visualization -->
        <div class="container has-text-centered">
          <h2 class="title is-5" style="margin-bottom: 1em;margin-top: 1em;">Example of 60 Primitive Tasks</h2>
        </div>

        <div class="columns is-vcentered">
          <div class="column has-text-centered">

            Visualization for   
            <div class="select is-small is-rounded">     
              <select id="interactive-menu" onchange="updateInteractive()">
              <option value="select_fruit" selected="selected">"Select Fruit Seriess"</option>
              <option value="select_toy">"Select Toy Series"</option>
              <option value="select_chemistry_tube">"Select Chemistry Tube Series"</option>
              <option value="add_condiment">"Add Condiment Series"</option>
              <option value="select_book">"Select Book Series"</option>
              <option value="select_painting">"Select Painting Series"</option>
              <option value="select_drink">"Select Drink Series"</option>
              <option value="insert_flower">"Insert Flower Series"</option>
              <!-- <option value=""> -->
              </select>
            </div>
          </div>

        </div>
        <div class="columns is-vcentered">
            <div class="column is-one-fourth">
              <div class="instruction-panel ">
              <p id="base-text">Pick the banana into the plate.</p>
              </div>
              <video id="video-base" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                <source src="./media/primitive_task/select_fruit/base.mp4" type="video/mp4">
              </video>
              <!-- ability and title here -->
              <p class="text-bottom-title">Mesh & Texture</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel">
                <p id="spatial-text">Pick the pear outside into the plate.</p>
                </div>
                <video id="video-spatial" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/spatial.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Spatial</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel ">
                  <p id="commonsense-text">Pick the fruit with Heat-clearing character into the plate.</p>
                </div>
                <video id="video-commonsense" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/commonsense.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Common Sense & World knowledge</p>
            </div>
            <div class="column is-one-fourth">
                <div class="instruction-panel ">
                  <p id="semantic-text">One apple one day, doctor keey away! I want one please.</p>
                </div>
                <video id="video-semantic" class="primitive-video" width="100%" height="100%" controls autoplay loop muted>
                    <source src="./media/primitive_task/select_fruit/semantic.mp4" type="video/mp4">
                </video>
                <p class="text-bottom-title">Semantic</p>
            </div>
          </div>

        </div>

        </div>
    </div>
  </div>
</section>

<!-- Tasks gallery -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-5" style="margin-bottom: 1em;">Example of Composite Tasks</h2>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <!-- <div class="columns is-multiline is-centered"> -->
        <div class="item">
          <div class="instruction-panel ">
            <p id="base-text">Instruction: Please rearrange the book by their publish year, early to late from left to right!</p>
          </div>
          <video class="galary-video" id="book_rearrange" poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/book_rearrange/book_rearrange.mp4"
                    type="video/mp4">
          </video>
          <p class="text-bottom-task-name">Book Rearrange</p>
        </div>
        <div class="item">
          <video id="cluster_billiards" poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_billiards/cluster_billiards.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
           <video id="cluster_book" poster="" autoplay muted loop height="100%" controlsList="nodownload">
             <source src="./media/composite_task/cluster_book/cluster_book.mp4"
                     type="video/mp4">
           </video>
         </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_drink/cluster_drink.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cluster_toy/cluster_toy.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/cook_dishes/cook_dishes.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/find_unseen_object/find_unseen_object.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/get_coffee/get_coffee_with_sugar.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/hammer_nail_and_hang_picture/demo_2_success_True.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/hammer_nail_and_hang_picture/demo_2_success_True.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/heat_food/heat_food.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/math_game/math_game.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/set_dining_table/set_dining_table.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/set_study_table/set_study_table.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/texas_holdem/texas_holdem.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/texas_holdem/texas_holdem_explore.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%" controlsList="nodownload">
            <source src="./media/composite_task/use_seesaw/use_seesaw.mp4"
                    type="video/mp4">
          </video>
        </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Evaluation</h2>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Experiment Result</h2>
      </div>
    </div>
  </div>
</section>

<section>
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="container has-text-centered">
        <h2 class="title is-3" style="margin-bottom: 1em;">Key Insight</h2>
      </div>
    </div>
  </div>
</section>
         
</body>
</html>